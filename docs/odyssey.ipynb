{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdoSGJp1fRAX"
   },
   "source": [
    "# CC 303 Final Project\n",
    "* Ryan Jinnette\n",
    "* 12/9/2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8AnYvRgiGrP",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Setup: Install Packages and Dependencies\n",
    "\n",
    "* Pandas: used for dataframe analysis and storing our counts and frequencies\n",
    "* Altair: used for graphing our pandas dataframes\n",
    "* Nltk: used for parsing the texts and classifying using natural language processing\n",
    "* nltk.corpus stopwords: used for getting rid of filler words like \"the\", \"and\", \"a\", etc.\n",
    "* Wordcloud: used for creating the nice wordclouds we see at the end of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fresh projects only\n",
    "# remove the hashtags before running the following cell\n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install altair\n",
    "# !pip install nltk\n",
    "# !pip install SpaCy\n",
    "# !pip install wordcloud\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "id": "Mt8aBdjXiGrR",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import nltk # ← new\n",
    "from nltk.corpus import stopwords as stop\n",
    "from nltk.stem import PorterStemmer as stemmer\n",
    "from nltk.stem import WordNetLemmatizer as lemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2FmhfVcK8LR"
   },
   "source": [
    "We need to tell nltk which \"models\" or settings we want to use for the following analysis. This only has to be done in the beginning per project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "id": "6exV5CucK-9b",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def download_models():\n",
    "    '''call this at the beginning of a session to install dependencies'''\n",
    "    nltk.download('punkt') # necessary for tokenization\n",
    "    nltk.download('wordnet') # necessary for lemmatization\n",
    "    nltk.download('stopwords') # necessary for removal of stop words\n",
    "    nltk.download('averaged_perceptron_tagger') # necessary for POS tagging\n",
    "    nltk.download('maxent_ne_chunker' ) # necessary for entity extraction\n",
    "    nltk.download('words')\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "    alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "# download_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHqcQyFRV3AC"
   },
   "source": [
    "### String Cleaning and Text Ingest\n",
    "\n",
    "Before we visualize any text, we need to do some normalization. Load our novel's text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "id": "AM0JtMJTV6Q7",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def ingest(text_name: str) -> str:\n",
    "    with open(text_name, 'r') as f:\n",
    "        story = f.read()\n",
    "    return story\n",
    "\n",
    "# story = ingest('Fagles.txt')\n",
    "# # display first 100 characters\n",
    "# print(story[:100]+\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwFmGp7bVuAX"
   },
   "source": [
    "At this point the entire story is loaded into one long string.\n",
    "\n",
    "💡 *Examine the character length of the story and look at the current state of the memory in the kernel:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "id": "LFmAtNugWCAG",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'story' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mstory\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'story' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(story))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PrbCFhmBrcV"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of turning a text into chunks that we can more easily work with. Typically tokenization refers to the separation and extraction of words. This process largely relies on the use of spaces and punctuation marks. The latter are typically included as tokens themselves.\n",
    "\n",
    "Suppose we have a sentence such as this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "TZMR1VHVHBQ_",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# sentence = \"It was a dark and stormy night; the rain fell in torrents—except at occasional intervals, when it was checked by a violent gust of wind which swept up the streets (for it is in London that our scene lies), rattling along the housetops, and fiercely agitating the scanty flame of the lamps that struggled against the darkness.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOQywkL8YbhT"
   },
   "source": [
    "Now, with nltk's `word_tokenize()` we can extract all tokens into a neat list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "Sg7EPmalYdU6",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# words = nltk.word_tokenize(sentence)\n",
    "# words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIJfRfKBP1GM"
   },
   "source": [
    "As you can see we also get the punctuation marks. These we can avoid with a different kind of tokenizer (e.g., the RegexpTokenizer) or by simply removing non-letter strings with Python's `isalpha()` method. This removes any token containing something else but letters, so if you expect that there are numbers, or embedded non-alpha characters, in your text you'll need to consider those seperately or use another method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "FLSVrCqA4INo",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# the list comprehension is extracting only words that contain exclusively letters.\n",
    "# onlywords = [word for word in words if word.isalpha()]\n",
    "\n",
    "# onlywords[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnEELBXVQlP_"
   },
   "source": [
    "Sometimes capitialization can change the mearning of the word, but for a broadstrokes natural language processing word cloud, we can remove capitialization to reduce the number of unique words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "tO7fZUmrZQYG",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# lowerwords = [word.lower() for word in onlywords]\n",
    "# lowerwords[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjhPIfXaBvBm"
   },
   "source": [
    "### Stemming & lemmatizing\n",
    "\n",
    "Words are often inflected to indicate plural, tense, case, etc. To get the word stem or lemma, you can apply stemming or lemmatization. The **stemmer** operates on a relatively robust, but simplistic rule set.  In contrast, lemmatization is more reliable in the linking different word variants of the same dictionary entry of the same word, a.k.a. lemma, but it's computationally more expensive.\n",
    "\n",
    "Let's compare them both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "XZRkr3CLiVb0",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# from nltk.stem import PorterStemmer as stemmer\n",
    "# from nltk.stem import WordNetLemmatizer as lemmatizer\n",
    "# from nltk.corpus import wordnet # for robust lemmatization\n",
    "\n",
    "# word = \"said\"\n",
    "\n",
    "# print(stemmer().stem(word))\n",
    "# print(lemmatizer().lemmatize(word, pos = wordnet.VERB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUImj32wbypb"
   },
   "source": [
    "To lemmatize we needed to indicate the word type via the second parameter `pos`. But how do we know that it's a verb?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfMficLW8kRN"
   },
   "source": [
    "### Part-of-speech tagging\n",
    "\n",
    "Words assume specific roles in sentences. POS tagging identifies these roles as the parts of speech, which roughly translates to word categories such as verbs, nouns, adjectives etc.\n",
    "\n",
    "To do POS tagging with NLTK, we need to first run the tokenization. So let's revisit the sentence from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "htHGnzUV8r1a",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# to save us some typing, we import these, so we can call them directly\n",
    "# from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# # first we tokenize then we pos_tag\n",
    "# sentence = pos_tag(word_tokenize(sentence))\n",
    "\n",
    "# sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Io0qGsrUatpB"
   },
   "source": [
    "This gives us a list of tuples, each of which contains the token again, plus the part of speech encoded in a tag. There is actually a good overview of the POS tags with brief definitions and examples [on Stack Overflow](https://stackoverflow.com/a/38264311).\n",
    "\n",
    "Again, let's go back to our full story, and extract the word from Alice, lemmatizing just the verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "anXYurrP90GF",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# same as above: first tokenize, then pos_tag\n",
    "pos = pos_tag(word_tokenize(story))\n",
    "\n",
    "# to keep things short & sweet, we define a function for lemmatizing verbs\n",
    "def lemmatize_verb (word):\n",
    "  return lemmatizer().lemmatize(word.lower(), pos = wordnet.VERB)\n",
    "\n",
    "# remember this form? the condition matches verbs, whose POS tag starts with a V\n",
    "verbs = [lemmatize_verb(word[0]) for word in pos if word[1][0]==\"V\"]\n",
    "\n",
    "# let's look at the first 50 verbs\n",
    "print(verbs[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki8jKbljdMfN"
   },
   "source": [
    "Woot! We just extracted all verbs from a story and normalized them! Awesome-sauce.\n",
    "Repeating the last step for nouns, we'll need to specify `lemmatize_noun()` and apply that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "OcBjKh0RdbSp",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# to keep things short & sweet, we define a function for lemmatizing verbs\n",
    "def lemmatize_noun (word):\n",
    "  return lemmatizer().lemmatize(word.lower(), pos = wordnet.NOUN)\n",
    "\n",
    "# remember this form? the condition matches verbs, whose POS tag starts with a V\n",
    "nouns = [lemmatize_noun(word[0]) for word in pos if word[1][0]==\"N\"]\n",
    "\n",
    "# let's look at the first 50 nouns\n",
    "print(nouns[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJZERqv-apOl"
   },
   "source": [
    "## 📄 2. Process\n",
    "\n",
    "Now we can turn a text into its components and distinguish these tokens as different word types. Let's proceed by extracting entities, removing irrelevant words, and finding the most frequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsKvqfOAFjUb"
   },
   "source": [
    "### Extract entity types\n",
    "\n",
    "Apart from identifying word types, we can distinguish between different entities that are mentioned in a text, such as persons, places, organizations, etc. This kind of text processing is also referred to as **Named Entity Recognition** (NER).\n",
    "\n",
    "For this step, we are straying from NLTK and use spaCy's statistical models on the English language. So first we import spaCy and load the English language model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "JDgNuT5feVHy",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6NruYSOcc7N"
   },
   "source": [
    "Let's find the named entities from Alice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "tWAUZ25kcd8v",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# carry out NLP processing\n",
    "doc = nlp(story)\n",
    "\n",
    "# get each the text and entity label of all word entities in the article\n",
    "entities = [ (e.text, e.label_) for e in doc.ents if e.text ]\n",
    "\n",
    "# see first 20 entities\n",
    "entities[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO3qLf3IekXB"
   },
   "source": [
    "Now all tokens that have been recognized as particular entities are extracted and associated with an entity type. Have a look at spaCy's [overview of NER tags](https://spacy.io/api/annotation#named-entities) to understand what they refer to.  These types of parts of speach taggers are typically trained on news stories.  You can see that the results are questionable for our novel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQSH6RYEBwn4"
   },
   "source": [
    "### Remove stop words\n",
    "\n",
    "The opposite of particularly interesting entitites are so-called stop words. They are very common and serve as short function words such as \"the\", \"is\", \"or\", \"at\". In text processing it can be useful to remove these frequent words to focus on those terms that are more specific to a given document.\n",
    "\n",
    "NLTK actually already includes stop words for several languages, including English.  I'm going to add the word \"chapter\" to the stop list for this case, since that is a visual que for the reader, not a integral part of the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "gollOAAnx077",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def define_stopwords() -> list:\n",
    "    stopwords = stop.words(\"english\")\n",
    "    stopwords.append('chapter')\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCzWmrZDiGsc",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "As a next step we remove the stop words from the short story to focus on those words that carry meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "IN7brHULytMC",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(story: str) -> list:\n",
    "    tokens = nltk.word_tokenize(story.lower())\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    without_stopwords = [word for word in words if word not in stopwords]\n",
    "    return words, without_stopwords\n",
    "    \n",
    "\n",
    "print(without_stopwords[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oabUvRRtB8sT"
   },
   "source": [
    "### Pack a bag of words\n",
    "\n",
    "A common representation of text documents is the bag-of-words model, which simply considers a given text as a set of words, disregarding sentence or document structure. Typically, a bag-of-words representation is combined with the frequency of words in a document. I'm also going to use the text with the stop words removed, assuming that they aren't interesting to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "PB7PNafI0cQ1",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def bag_of_words(words: list[str]) -> list:\n",
    "    bow = {}\n",
    "    for word in words:\n",
    "      bow[word] = words.count(word)\n",
    "    words_frequency = sorted(bow.items(), key=lambda x: x[1], reverse=True)\n",
    "    return words_frequency\n",
    "\n",
    "# print(words_frequency[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYtf_Oica69B"
   },
   "source": [
    "## 🥗 3. Present\n",
    "\n",
    "Now let's turn all these words into visualizations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b0a9VJvCZz_"
   },
   "source": [
    "### Word cloud\n",
    "\n",
    "For text visualization, one technique has reached a lot of attention, despite its limited perceptual and analytical qualities. The word cloud (a.k.a. tag cloud) emerged in the golden age of Web 2.0 (the 2000s) and probably succeeded due to its simplicity in terms of interpretation and implementation: the more frequent a word, the larger the font size. Altair itself actually does not support word clouds, so we resort to a specific `wordcloud` generator and use `matplotlib` to render the images. The wordcloud library is extra convenient, it just takes the raw text as input:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "89g7cPgq8Ia4",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def create_wordcloud(story:str):\n",
    "    wc = WordCloud(width=500, height=500, background_color=\"white\").generate(story)\n",
    "    # display the generated image:\n",
    "    my_dpi = 72\n",
    "    plt.figure(figsize = (500/my_dpi, 500/my_dpi), dpi=my_dpi)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU9RZ25ZiGsk",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "💡 *The word cloud library actually gives a lot of options for [customization](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud.WordCloud). You can change the colors, fonts, sizes, and keep the stopwords*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myPOLf6hGJRL"
   },
   "source": [
    "### Common words\n",
    "\n",
    "We shall move on to more precise representations of text. For this we will revisit an arguably mundane, but quite effective visualization technique: we draw a barchart of the most frequent words (excluding the stop words, if you have done the pencil activity in the section on packing a bag of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "VkaGlIB15GiA",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_common_words(words_frequency: list) -> pd.DataFrame:\n",
    "    # first we create a dataframe from the word frequencies\n",
    "    df = pd.DataFrame(words_frequency, columns=['word', 'count'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_top_words(df: pd.DataFrame):\n",
    "    # we want to focus just on the top 20 words\n",
    "    df_top = df[:50]\n",
    "    # df_top_100 = df[:100] # use this later\n",
    "    \n",
    "    # draw horizontal barchart\n",
    "    alt.Chart(df_top).mark_bar().encode(\n",
    "      x = 'count:Q',\n",
    "      y = alt.Y('word:N', sort = '-x')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odyPoQxxiGsn",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### All words by type\n",
    "\n",
    "Through POS tagging we are able to identify the different word types, such as nouns, verbs, adjectives, adverbs, and several others. So let's do exactly this and distinguish between these common word types for the story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "1UnCY5H0iGsn",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "# first we extract all words and their types (a.k.a. parts-of-speech or POS)\n",
    "pos = pos_tag(word_tokenize(story))\n",
    "\n",
    "# we will be collecting words and types in lists of the same length\n",
    "words = []\n",
    "types = []\n",
    "\n",
    "# iterate over all entries in the pos list (generated above)\n",
    "for p in pos:\n",
    "  # get the word and turn it into lowercase\n",
    "  word = p[0].lower()\n",
    "  # get the word's type\n",
    "  tag = p[1]\n",
    "\n",
    "  # for this analysis we remove entries that contain punctuation or numbers\n",
    "  # and we also ignore the stopwords (sorry: the, and, or, etc!)\n",
    "  if word.isalpha() and word not in stopwords:\n",
    "    # first we add this word to the words list\n",
    "    words.append(word)\n",
    "    # then we add its word type to types list, based on the 1st letter of the pos tag\n",
    "    # note that we access letters in a string, like entries in a list\n",
    "    if   (tag[0]==\"J\"): types.append(\"Adjective\")\n",
    "    elif (tag[0]==\"N\"): types.append(\"Noun\")\n",
    "    elif (tag[0]==\"R\"): types.append(\"Adverb\")\n",
    "    elif (tag[0]==\"V\"): types.append(\"Verb\")\n",
    "    # there are many more word types, we simply subsume them under 'other'\n",
    "    else: types.append(\"Other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5nYUUKCmdD1"
   },
   "source": [
    "💡 *This is a good point to check what we generated. Take a look at the two lists we created:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FM2lQQp7BLar"
   },
   "outputs": [],
   "source": [
    "words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LHKafOuBOtt"
   },
   "outputs": [],
   "source": [
    "types[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQMVflYNm222"
   },
   "source": [
    "With this information, we can now create two coordinated charts: one representing the frequency of the different word types and the other displaying the frequency of all words (given the current selection). But first things first: we need to create a dataframe with only the most popular 100 words by frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnPlYEW_4REr"
   },
   "outputs": [],
   "source": [
    "# with the two lists of the same length, we create a dataframe with a dictionary,\n",
    "# of which the keys will become the column labels\n",
    "df = pd.DataFrame({\"word\": words, \"type\": types })\n",
    "\n",
    "# Filter out only the top 100 words by frequency\n",
    "index = df['word'].isin(df_top_100['word'])\n",
    "df_pared = df[index].reset_index(drop=True)\n",
    "len(df_pared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLmi2OL_mbLc"
   },
   "outputs": [],
   "source": [
    "# along the type column, we want to support a filter selection\n",
    "selection = alt.selection(type=\"multi\", fields=['type'])\n",
    "\n",
    "# we create a composite chart consisting of two sub-charts\n",
    "# the base holds it together and acts as the concierge taking care of the data\n",
    "base = alt.Chart(df_pared)\n",
    "\n",
    "# this shows the types, note that we rely on Altair's aggregation prowess\n",
    "chart1 = base.mark_bar().encode(\n",
    "  x = alt.Y('type:N'),\n",
    "  y = alt.X('count()'),\n",
    "  # when a bar is selected, the others are displayed with reduced opacity\n",
    "  opacity=alt.condition(selection, alt.value(1), alt.value(.25)),\n",
    ").add_selection(selection)\n",
    "\n",
    "# this chart reacts to the selection made in the left/above chart\n",
    "chart2 = base.mark_bar(width=5).encode(\n",
    "  x = 'word:N',\n",
    "  y = alt.Y('count()'),\n",
    ").transform_filter(selection)\n",
    "\n",
    "chart1 | chart2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQb3_h_bCOxI"
   },
   "source": [
    "### Keyword in context\n",
    "\n",
    "Last but not least, it can be quite gratifying to see words in their original context. KWIC is a tried and tested method just for that purpose. Let's build one from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDnsa-4iiGsq",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": true
   },
   "outputs": [],
   "source": [
    "import re # regular expressions, we will need them to search through the text\n",
    "# the following we need, to display a text input field and make it interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# we move all line breaks with spaces, to not mess up the display (you'll see)\n",
    "text = story.replace(\"\\n\", \" \")\n",
    "\n",
    "# create a search box …\n",
    "search_box = widgets.Text(placeholder='Enter search term', description='Search:')\n",
    "# … and make it appear\n",
    "display(search_box)\n",
    "\n",
    "# this function is triggered when a search query is entered\n",
    "def f(sender):\n",
    "  # we get the query's text value\n",
    "  query = search_box.value\n",
    "\n",
    "  # this is the window of characters displayed both sides\n",
    "  span = 40 - int(len(query)/2)\n",
    "\n",
    "  # for subsequent queries, we clear the output\n",
    "  clear_output(wait=True)\n",
    "  # which also removes the search box, so we return it\n",
    "  display(search_box)\n",
    "\n",
    "  # when the query is too short, we do not proceed and warn the user/reader\n",
    "  if (len(query)<2):\n",
    "    print(\"\\nPlease enter a longer query\\n\")\n",
    "    return\n",
    "\n",
    "  # and find all the start positions of matches in the text\n",
    "  starts = [m.start() for m in re.finditer(query, text)]\n",
    "\n",
    "  # if there are no matches, we also tell the user/reader\n",
    "  if (len(starts)==0):\n",
    "    print(\"\\nSorry, but there are no matches for your query\\n\")\n",
    "    return\n",
    "\n",
    "  # we go through all the start positions\n",
    "  for start in starts:\n",
    "    # determine the end position, based on the query's length\n",
    "    end = start+len(query)\n",
    "\n",
    "    # we get the string left and right of the match\n",
    "    # rjust returns a right-justified string, if there are few letters left of match\n",
    "    left = text[max(0, start-span):start].rjust(span)\n",
    "    match = text[start:end]\n",
    "    right = text[end:end+span]\n",
    "\n",
    "    # we print left and right context with the actual match in the middle\n",
    "    print(left+match+right)\n",
    "\n",
    "# the function f is linked with searchbox' on_submit event\n",
    "search_box.on_submit(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbpLZ6mO4REs"
   },
   "source": [
    "Try searching for `Alice` or `Rabbit`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dj0EUni4REs"
   },
   "source": [
    "## Your Turn\n",
    "\n",
    "  1. Find all of the named entities in the file `elsa_peretti_obit.txt`. Do any of the classifications seem wrong to you? Why or why not?\n",
    "  2. From the `elsa_peretti_obit.txt` text file, create a horizonal bar chart of word frequencies as we did above, but sort it by values. [This example will help](https://altair-viz.github.io/gallery/bar_chart_sorted.html)\n",
    "  3. Re-create the interactive graph with word type frequencies and word frequencies by selection from the tutorial, except this time use the `elsa_peretti_obit.txt` text file instead of the Alice in Wonderland text file. Sort the word frequencies from highest on left to lowest on the right.\n",
    "  4. Crete a different word cloud for Alice's story, in the shape of Alice and the white rabbit.  We provided a mask, `alice_mask.png`. [This documentation](https://amueller.github.io/word_cloud/auto_examples/masked.html) will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JlvBr_WuTBT"
   },
   "outputs": [],
   "source": [
    "#request = requests.get('https://gist.githubusercontent.com/isha211/f1ce8a7020230205099399f7dc8edb30/raw/dfda5a993eaec33fdc4dd839556dfaab41abdaf7/elsa_peretti_obit.txt')\n",
    "obit_text = request.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXfC7KUB8LOD"
   },
   "outputs": [],
   "source": [
    "# Question 1\n",
    "# carry out NLP processing\n",
    "\n",
    "# get the text and entity label of all word entities in the article\n",
    "\n",
    "# print the entity classifications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaTnDJopt1ej"
   },
   "source": [
    "➡️ YOUR ANSWER HERE: Do any of the classifications seem wrong to you? Why or why not? ⬅️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mhPaKOl4REs"
   },
   "outputs": [],
   "source": [
    "# Question 2\n",
    "# read in the text file\n",
    "\n",
    "# get tokens which contain letters and are longer than two characters\n",
    "\n",
    "# remove stopwords\n",
    "\n",
    "# pack a bag of words and get word-frequency tuples\n",
    "\n",
    "# draw a sorted, horizontal barchart for the top 20 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTh1lSmd4REt"
   },
   "outputs": [],
   "source": [
    "# Question 3\n",
    "# refer to the tutorial for how to extract word types for the obituary\n",
    "\n",
    "# get only the top 100 words of the obituary by frequency\n",
    "\n",
    "# refer to the interactive chart in the tutorial to build a similar chart here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiDb3jgD4REt"
   },
   "outputs": [],
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmSo5bB8As7s"
   },
   "source": [
    "## Sources\n",
    "\n",
    "Tutorials\n",
    "- [A Complete Exploratory Data Analysis and Visualization for Text Data](https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a)\n",
    "- [Named Entity Recognition with NLTK and SpaCy](https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da)\n",
    "- [What are all possible pos tags of NLTK? - Stack Overflow](https://stackoverflow.com/a/38264311)\n",
    "\n",
    "Documentation\n",
    "- [Natural Language Toolkit — NLTK 3.5 documentation](https://www.nltk.org)\n",
    "- [spaCy API Documentation - Architecture](https://spacy.io/api)\n",
    "- [WordCloud for Python documentation](https://amueller.github.io/word_cloud/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "nteract": {
   "version": "0.23.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
